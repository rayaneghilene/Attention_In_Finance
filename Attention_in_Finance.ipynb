{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayaneghilene/Attention_In_Finance/blob/main/Attention_in_Finance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fh_1VOxhSI7",
        "outputId": "91159a06-75c1-4221-887c-1e99a1474384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/776.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/776.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.3/776.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.10.0 pytorch-lightning-2.1.2 torchmetrics-1.2.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.0\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install wandb\n",
        "!pip install torchvision\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0-iNSH47r3uu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d19d9dfe-2011-4a92-dc99-c06ef3125f1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "USE_COLAB = True\n",
        "CONTENT_DIR = \"/content\" if USE_COLAB else \".\"\n",
        "\n",
        "import wandb\n",
        "if not wandb.login():\n",
        "    raise ValueError(\"WandDB authentification failed.\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(42)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
        "from typing import Optional, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "def _get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAV2huZur75y"
      },
      "source": [
        "# Transformer : A Model Used Everywhere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGZZ0bSCsCp8"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Goal of this lab :     \n",
        "\n",
        "\n",
        "*   Understand and code basic Transformer\n",
        "*   Introduction to Finance\n",
        "*  Understand your knowledge of deep learning.\n",
        "* Become Rich ?\n",
        "\n",
        "**Disclaimer : ANY NON ILLUSTRATED ANALYSIS WILL LEAD TO A 0 TO THE GIVEN PART**\n",
        "\n",
        "<img src =\"https://i.imgflip.com/6g1fwb.jpg?a463272\" height = 200>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RqzZHrcNQx5"
      },
      "source": [
        "We have seen through multiple labs industrial applications of Deep Learning models in Computer Vision. Now let's look at another task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7VWYvDMNpNt"
      },
      "source": [
        "# Transformer : Definitions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2nagjfnOO9p"
      },
      "source": [
        "## Model\n",
        "\n",
        "\n",
        "The transformer is a  network architecture based on attention mechanisms. It consists of an encoder-decoder architecture:\n",
        "* the encoder maps an input sequence of symbol representations $(x_1,…,x_n)$ to a sequence $z=(z_1,…,z_n)$\n",
        "* the decoder, given $z$ , generates an output sequence $(y_1,…,y_m)$.\n",
        "\n",
        "At each time step, the model consumes the previously generated symbols as additional input when generating the next (it is auto-regressive)\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" width= 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HJarJb4OuV-"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "Transformers work on a sequence of Data. Order does matter.\n",
        "Example :  \n",
        "\n",
        "\n",
        "> **Vivre pour manger =! Manger pour vivre.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For recurrent networks (RNN, LSTM, GRU), the word positions are implicitly embedded inside the model since they are processed sequentially. The Transformer doesn't process sequentially the input. How can we specify the position of a Token ?\n",
        "The solution of the authors was to add a vector representing the position of the words to the embedding vectors:\n",
        "\n",
        "\n",
        "> **THE POSITIONAL ENCODING**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQKt23MiP10x"
      },
      "source": [
        "## Attention : The reason of this model\n",
        "\n",
        "Attention , self-attention and multi-head attention are the main component of this model. They let the model know that specific regions or tokens are interesting. They let the model take care of specific parts of the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBltd6vNIiG"
      },
      "source": [
        "# Finance : Is attention all you need ?\n",
        "\n",
        "In this lab, we will try to create a model able to predict trends in Stock Values. This might be a foundation for a trading bot. We will use a Transformer Encoder to perform this task.\n",
        "\n",
        "Feel free to download any stock you want.\n",
        "Some examples of stock :\n",
        "* S&P500 : https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC , https://www.investing.com/indices/us-spx-500-historical-data, https://www.nasdaq.com/market-activity/index/spx/historical\n",
        "* Bitcoin : https://finance.yahoo.com/quote/BTC-USD/history?p=BTC-USD\n",
        "* Gold : https://finance.yahoo.com/quote/GC%3DF/history?p=GC%3DF\n",
        "* ...\n",
        "\n",
        "To download your csv file, go on the Historical Data tab, choose a Time Period and Download your file.\n",
        "\n",
        "**Important : Make sure that you have a historical data of a long period of time.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMvRTGC20mSu"
      },
      "source": [
        "### I - Time Series : A Different Dataset\n",
        "\n",
        "Time Series are a series of data points indexed by time. We often treat them by sequence of equally spaced points in time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7QlOkOU1RY9"
      },
      "source": [
        "#### a - EDA : What Trends in your stock ?\n",
        "\n",
        "Let's read your stock file. As a Data Scientist, you will be asked to explore data before doing anything. Data Exploration is needed for data understanding and preprocessing.\n",
        "* Using Pandas perform a full Exploratory Dataset Analysis.\n",
        "\n",
        "Feel free to read the documentation of each library. It will be needed to perform anything\n",
        "\n",
        "Is expected :\n",
        "* A description of the stock\n",
        "* A summary of the features\n",
        "* What could be the trend for next week ?\n",
        "* Plots of the useful features.\n",
        "* Correlation between features.\n",
        "\n",
        "Some useful libraries :    \n",
        "* To read a csv file : use **pandas** library and read_csv method\n",
        "* **seaborn** is a useful library that processes a pandas dataframe\n",
        "* **matplotlib** is your old friend\n",
        "\n",
        "Some useful methods :    \n",
        "* In pandas :\n",
        "  * head()\n",
        "  * describe()\n",
        "* In seaborn:\n",
        " * displot()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gsNFiWHp1NgU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df= pd.read_csv('/content/all_stocks_5yr.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pYwVJDsEiDUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "753c1cb7-29a3-4aeb-b510-3f9809448fdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date   open   high    low  close      volume Name\n",
              "0  2013-02-08  15.07  15.12  14.63  14.75   8407500.0  AAL\n",
              "1  2013-02-11  14.89  15.01  14.26  14.46   8882000.0  AAL\n",
              "2  2013-02-12  14.45  14.51  14.10  14.27   8126000.0  AAL\n",
              "3  2013-02-13  14.30  14.94  14.25  14.66  10259500.0  AAL\n",
              "4  2013-02-14  14.94  14.96  13.16  13.99  31879900.0  AAL"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3bb0ccb-36d8-4875-93eb-db6d2b2561b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2013-02-08</td>\n",
              "      <td>15.07</td>\n",
              "      <td>15.12</td>\n",
              "      <td>14.63</td>\n",
              "      <td>14.75</td>\n",
              "      <td>8407500.0</td>\n",
              "      <td>AAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2013-02-11</td>\n",
              "      <td>14.89</td>\n",
              "      <td>15.01</td>\n",
              "      <td>14.26</td>\n",
              "      <td>14.46</td>\n",
              "      <td>8882000.0</td>\n",
              "      <td>AAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013-02-12</td>\n",
              "      <td>14.45</td>\n",
              "      <td>14.51</td>\n",
              "      <td>14.10</td>\n",
              "      <td>14.27</td>\n",
              "      <td>8126000.0</td>\n",
              "      <td>AAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013-02-13</td>\n",
              "      <td>14.30</td>\n",
              "      <td>14.94</td>\n",
              "      <td>14.25</td>\n",
              "      <td>14.66</td>\n",
              "      <td>10259500.0</td>\n",
              "      <td>AAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013-02-14</td>\n",
              "      <td>14.94</td>\n",
              "      <td>14.96</td>\n",
              "      <td>13.16</td>\n",
              "      <td>13.99</td>\n",
              "      <td>31879900.0</td>\n",
              "      <td>AAL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3bb0ccb-36d8-4875-93eb-db6d2b2561b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3bb0ccb-36d8-4875-93eb-db6d2b2561b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3bb0ccb-36d8-4875-93eb-db6d2b2561b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-95417573-f310-401d-ba35-9831a6c723a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-95417573-f310-401d-ba35-9831a6c723a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-95417573-f310-401d-ba35-9831a6c723a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya7-5jyUqv-c"
      },
      "source": [
        "#### b - Creating a Stock Dataset\n",
        "\n",
        "As you might have seen, multiple features are available in the dataset. We will use the attention mecanism to process 5 of the given features. We will use the following features :    \n",
        "* Open\n",
        "* High\n",
        "* Low\n",
        "* Close\n",
        "* Volume\n",
        "\n",
        "We are going to create a Dataset that is conform to Pytorch Dataset class.\n",
        "The Dataset should take as input :\n",
        "*    Dataframe(s?)\n",
        "*    $N_{window}$ : length of the sequence\n",
        "\n",
        "\n",
        "We're feeding the model Time Series which are sequential datas. Each data $p_{i}$ is part of a sequence of length $N_{window}$. We want to predict what could happen after that data. We are sending the data thanks to a sliding non overlapping window of size $N_{window}$.\n",
        "\n",
        "In fact, we will be sending $N_{window}$ sequences to the model to learn how to approximate  $f( W_t) \\approx W_{t+1}$ \\\n",
        "where :\n",
        "\n",
        "\n",
        "$W_t = (p_{t_{w}}, p_{t_{w+1}}, \\dots, p_{(t+1)_{w-1}})$\n",
        "\n",
        "We want the dataset to return the following:\n",
        "for\n",
        "\n",
        "*    $N_{window}$ = 3\n",
        "\n",
        "\n",
        "$\\text{Input}_1 = [p_0, p_1, p_2], \\text{Label}_1 = [p_3, p_4, p_5]$ \\\n",
        "\n",
        "\n",
        "$\\text{Input}_3 = [p_6, p_7, p_8], \\text{Label}_3 = [p_9, p_{10}, p_{11}]$\n",
        "\n",
        "__getitem__ method should return a dictionnary where :\n",
        "*   dict['input'] : is a list of $N_{window}$ input values\n",
        "* dict['label'] : is a list of $N_{window}$ target values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "qRJG6afmrdPD"
      },
      "outputs": [],
      "source": [
        "class StockDataset(Dataset):\n",
        "\n",
        "  def __init__(self, df,N_window,normalized = True,num_steps =1):\n",
        "      self.df = df\n",
        "      self.df.dropna(how='any', axis=0,inplace= True)\n",
        "      self.N_window = N_window\n",
        "      self.num_steps =num_steps\n",
        "      self.normalized = normalized\n",
        "      self.X, self.y = self.process_df()\n",
        "\n",
        "  def process_df(self):\n",
        "      \"\"\"\n",
        "      process method should return X,y:\n",
        "      * X is an array of num_steps*N_windows input values\n",
        "      * y is an array of corresponding target values\n",
        "      \"\"\"\n",
        "      scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "      data_raw= self.df.to_numpy()\n",
        "      close = self.df['close'].to_numpy()\n",
        "      open = self.df['open'].to_numpy()\n",
        "      high = self.df['high'].to_numpy()\n",
        "      low = \tself.df['low'].to_numpy()\n",
        "      volume = self.df['volume'].to_numpy()\n",
        "\n",
        "      if self.normalized :\n",
        "        close = scaler.fit_transform(close.reshape(-1, 1))\n",
        "        open = scaler.fit_transform(open.reshape(-1, 1))\n",
        "        high = scaler.fit_transform(high.reshape(-1, 1))\n",
        "        low = \tscaler.fit_transform(low.reshape(-1, 1))\n",
        "        volume = scaler.fit_transform(volume.reshape(-1, 1))\n",
        "\n",
        "      data_raw= np.stack([close,open,high,low,volume])\n",
        "      assert len(close)==len(open)==len(high)==len(low)==len(volume)\n",
        "\n",
        "\n",
        "\n",
        "      # TODO : Create a list of sequences of N_window elements\n",
        "\n",
        "      seq = [data_raw[:, i:i + self.N_window + self.num_steps] for i in range(len(data_raw[0]) - self.N_window - self.num_steps + 1)]\n",
        "      #print(\"data raw shape is\", data_raw[0])\n",
        "      X = np.array(seq)[:, :, :self.N_window]\n",
        "      y = np.array(seq)[:, :, self.N_window:self.N_window +self.num_steps]\n",
        "\n",
        "      return X, y\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      '''\n",
        "      Be careful on your len because of the overlapping issues\n",
        "      '''\n",
        "      # TODO : What is the len of the dataset ?\n",
        "\n",
        "      ############################################# ANSEWER ################################################\n",
        "      #print(\"the length of the dataset is\", len(self.X))\n",
        "      #the length of the dataset is 350717\n",
        "      ######################################################################################################\n",
        "\n",
        "      return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "      \"\"\"\n",
        "      __getitem__ method should return a dictionnary where :\n",
        "      * dict['input'] : is a list of num_{steps} lists of N_{window} elements\n",
        "      * dict['label'] : is a list of N_{window} target value\n",
        "      \"\"\"\n",
        "\n",
        "      if idx >= len(self):\n",
        "        raise IndexError(f\"Index {idx} is out of bounds for the dataset with length {len(self)}\")\n",
        "\n",
        "      # TODO : Return one element\n",
        "      x = self.X[idx]\n",
        "      #print(\"the value of x is:\", x)\n",
        "      y = self.y[idx]\n",
        "      data_dict = {'input': x.squeeze(-1), 'label': y.squeeze(-1)}\n",
        "      return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "tQh8P6nauOUM"
      },
      "outputs": [],
      "source": [
        "# TODO : Verify that your Dataset is correct.\n",
        "dataset = StockDataset(df = df,\n",
        "                       N_window = 3,\n",
        "                       normalized = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "CS-UISRq-JxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee24b482-4db3-43b4-a736-aa10e41aa4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.9822397  -0.9825019  -0.98196371]\n",
            " [-0.98201617 -0.98261247 -0.98281575]\n",
            " [-0.9821962  -0.98286451 -0.98228977]\n",
            " [-0.98238226 -0.98260317 -0.98239607]\n",
            " [-0.95881647 -0.96232188 -0.95242928]]\n",
            "[[-0.98288829 -0.9821845  -0.9825157  -0.98379908 -0.98374388 -0.98346788\n",
            "  -0.98422687]\n",
            " [-0.98194841 -0.98331718 -0.98277509 -0.98299193 -0.9837373  -0.98380506\n",
            "  -0.98376441]\n",
            " [-0.98226303 -0.98273085 -0.98279768 -0.98319867 -0.98361302 -0.98408084\n",
            "  -0.98386698]\n",
            " [-0.98390103 -0.98283789 -0.98263079 -0.98391484 -0.98426001 -0.983832\n",
            "  -0.98412194]\n",
            " [-0.85217995 -0.92753666 -0.94735246 -0.93172275 -0.94472015 -0.97184864\n",
            "  -0.96667861]]\n"
          ]
        }
      ],
      "source": [
        "print(dataset.__getitem__(1)[\"input\"])\n",
        "print(dataset.__getitem__(1)[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__getitem__(6)[\"input\"]#.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wk5RRfizfN8",
        "outputId": "bc67f6bd-3336-4c8c-b187-ae47a3159818"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.9825157 , -0.98379908, -0.98374388],\n",
              "       [-0.98277509, -0.98299193, -0.9837373 ],\n",
              "       [-0.98279768, -0.98319867, -0.98361302],\n",
              "       [-0.98263079, -0.98391484, -0.98426001],\n",
              "       [-0.94735246, -0.93172275, -0.94472015]])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoD75GvwtfT_"
      },
      "source": [
        "At this moment, each samples will be a dictionnary with a sequence. However, one important feature is hidden in our stock price but we need a proper way to use it.\n",
        "* What feature are we talking about ?\n",
        "\n",
        "\n",
        "*We're referring to the nature of the stock price data (temporal/sequential). In time series data, each data point is associated with a specific point in time, and the sequence of data points reflects the temporal evolution of the stock prices.*\n",
        "\n",
        "*By using a sliding window approach to create sequences (Input-Label pairs) from the time series data, we are preserving the temporal aspect.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zY-G3SrKxc"
      },
      "source": [
        "#### c- Creating the Lightning DataModule\n",
        "\n",
        "As usual create a Lightning Datamodule that encompasses everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "53MZMTbbrYRT"
      },
      "outputs": [],
      "source": [
        "class StockDataModule(pl.LightningDataModule):\n",
        "    def __init__(self,df,N_window, normalized, batch_size):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.N_window = N_window\n",
        "        self.normalized = normalized\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage):\n",
        "        # First stage is 'fit' (or None)\n",
        "\n",
        "        # TODO : Do we shuffle the datasets ? Why ?\n",
        "\n",
        "        ############################################# ANSEWER ################################################\n",
        "\n",
        "        ##  WE SHOULD NOT SHUFFLE OUR DATA! because shuffling the data removes the temporal aspect of the data,\n",
        "        #and thus we wouldn't be able to predict stock prices.\n",
        "\n",
        "        ######################################################################################################\n",
        "\n",
        "        X_train, X_test = train_test_split(self.df, shuffle=False)\n",
        "        X_train, X_valid = train_test_split(X_train, shuffle=False)\n",
        "\n",
        "\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # We create a validation split to watch the training.\n",
        "            # TODO : As usual\n",
        "            self.stock_train =  StockDataset(df=X_train ,N_window = self.N_window, normalized=True)\n",
        "            self.stock_valid =  StockDataset(df=X_valid ,N_window = self.N_window, normalized=True)\n",
        "\n",
        "        # Second stage is 'test'\n",
        "        if stage == \"test\" or stage is None:\n",
        "            # TODO : As usual\n",
        "            self.stock_test =  StockDataset(X_test ,N_window = self.N_window, normalized=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.stock_train, batch_size=self.batch_size, num_workers=1, shuffle=False)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.stock_valid, self.batch_size,num_workers=1, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.stock_test,self.batch_size,num_workers=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "tpRHAcYNda6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa4ef71-31c0-4a64-b4ed-1dba1e95b069"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.9818, -0.9822, -0.9825],\n",
              "         [-0.9818, -0.9820, -0.9826],\n",
              "         [-0.9820, -0.9822, -0.9829],\n",
              "         [-0.9819, -0.9824, -0.9826],\n",
              "         [-0.9552, -0.9526, -0.9567]],\n",
              "\n",
              "        [[-0.9822, -0.9825, -0.9820],\n",
              "         [-0.9820, -0.9826, -0.9828],\n",
              "         [-0.9822, -0.9829, -0.9823],\n",
              "         [-0.9824, -0.9826, -0.9824],\n",
              "         [-0.9526, -0.9567, -0.9453]],\n",
              "\n",
              "        [[-0.9825, -0.9820, -0.9829],\n",
              "         [-0.9826, -0.9828, -0.9819],\n",
              "         [-0.9829, -0.9823, -0.9823],\n",
              "         [-0.9826, -0.9824, -0.9839],\n",
              "         [-0.9567, -0.9453, -0.8300]],\n",
              "\n",
              "        [[-0.9820, -0.9829, -0.9822],\n",
              "         [-0.9828, -0.9819, -0.9833],\n",
              "         [-0.9823, -0.9823, -0.9827],\n",
              "         [-0.9824, -0.9839, -0.9828],\n",
              "         [-0.9453, -0.8300, -0.9167]],\n",
              "\n",
              "        [[-0.9829, -0.9822, -0.9825],\n",
              "         [-0.9819, -0.9833, -0.9828],\n",
              "         [-0.9823, -0.9827, -0.9828],\n",
              "         [-0.9839, -0.9828, -0.9826],\n",
              "         [-0.8300, -0.9167, -0.9395]]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ],
      "source": [
        "dl = StockDataModule(df= df, N_window = 3, normalized=True, batch_size= 5)\n",
        "dl.setup(None)\n",
        "input = next(iter(dl.train_dataloader()))\n",
        "input[\"input\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ilwKU725epE"
      },
      "source": [
        "### II - Positional Encoding : Incorporating Time to the features with Time2Vector\n",
        "\n",
        "Transformers use a posiional encoding to provide a sense of word order in a sequence. However, these positional encodings doesn't provide any sens of time. In that way, Time2Vector is a model-agnostic vector representation for time. The main idea of this vector is that :\n",
        "* a meaningful representation of time has to include both periodic and non-periodic patterns.\n",
        "* a time representation should have an invariance to time rescaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nt2LXf58vHu"
      },
      "source": [
        "\n",
        "Specifically, a Time2Vec Layer is defined as :    \n",
        "<img src =\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABQAAAADkCAIAAABIax6TAAAgAElEQVR4nO3dd3xT5fs+8KtsQfbeqMiQDbKHqCAKiAxZgmwRBFSGCCJDlkxFEFQ2MmXVMj6A7C1LpmxkyN5bBFp+f1y/J9/YlvQkzWh7rvcffSVpcs6TNjnJc+77ue+gJ0+eQERERERERCSuixfoAYiIiIiIiIj4gybAIiIiIiIiYguaAIuIiIiIiIgtaAIsIiIiIiIitqAJsIiIiIiIiNiCJsAiIiIiIiJiC5oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImILmgCLiIiIiIiILWgCLCIiIiIiIragCbCIiIiIiIjYgibAIiIiIiIiYguaAIuIiIiIiIgtaAIsIiIiIiIitqAJsIiIiIiIiNiCJsAiIiIiIiJiC5oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImILmgCLiIiIiIiILWgCLCIiIiIiIragCbCIiIiIiIjYgibAIiIiIiIiYguaAIuIiIiIiIgtaAIsIiIiIiIitqAJsIiIiIiIiNiCJsAiIiIiIiJiC5oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImILCQI9ABGRAAgLCwOwZMkSAIcOHQLw0UcfAUiePHlgByYiIiIivhP05MmTQI9BRMSv1q1b98knnwDYt2+f48bdu3cDKFq0aMCGJSIiIiI+phRoERERERERsQWlQIuILVy/fh1Au3btAMybN483Zs6cGcCQIUOg2K+IiIiIDSgCLCIiIiIiIragCLCIxHFHjx4FULNmTQDHjh3jjZ06dQIwaNAgqPCViIiIiG0oAiwiIiIiIiK2oAiwiMRZv//+O4Dq1asDuHHjBoAkSZIAmDBhQtOmTQM7NhERERHxP0WARURERERExBbUB1hE4qBr164BKFKkCIBz584BiBcvHoDg4GAAtWrVCujoRERsbfXq1QBCQ0OrVq0KICgoyONNPXz4EMCOHTsA/P333zCZPvnz58+bN69XRisicYwiwCIiIiIiImILWgMsInHNkydPmjdvDhP7JRZ8VuxXRCSA+vbtC6B///682rFjRwBjxoxxdzuPHz8ePHgwgFmzZgHImTMngG3btgG4desW7/PKK68AmDt3LoAMGTJ4Y/jRwjYEM2bMAPDmm2+WLVs20COKxU6cOAFg2rRpAA4dOgTg5s2bALJnz16jRg0A/Ml0AJFwlAItInHNokWL3nnnHcfVfPnyAdi/fz+ABAl01k9EJGCKFSsGYM+ePbyaPXt2AGfOnLG+hbCwMAC1atXavn07gL179wLInDkzzEnPUqVKATh//jzv37ZtWwA//fST156D+65evQqAKdnXr18HkC5dusuXLyN66d829OjRI3YxnD59OkxHw2bNmgFImTIlgG3btvHMCFdCzZ49G0CZMmUCN2SJiZQCLSIiIiIiIragYIiIxDU84+vQpUsXKPYrIhIDvPnmm3CKADNP1S3jxo0DsHTp0g4dOsDEfilr1qwAfv31VwAVK1b8999/YYLMgcUcXcZ+KWvWrIr9uuXx48cA3nzzzQ0bNsDUUatUqVK4u9WtW5cvqmrVqgGoWLEigDlz5gCoV6+ef4csMZciwCIiIiIiImILWgMsInHHvXv3AGTMmJEXeH6di6/SpEkT2LGJH8ybNw9mJdgbb7wR6OGISHj82rls2TJerl69OtxcB8uyDkeOHBk5ciRMjk9E+/fv59Jixpzjx48f3aFHA9ctswAYezV99dVXLNwlFg0fPhxA9+7dO3fuDOCbb75xfX+uAc6VKxfMq+7gwYMAcuTI4euhSsynCLCIiIiIiIjYgiLAIhJ3rFixAuZ8P4Dnn38eplmC2AGXe6VLlw5AcHBwoIcjIt60bt06AK+++iqv/vjjjwA+/PDDQI5JfO/s2bMwNbTv379/5MgRAHny5LHy2NatWwOYPHkyAPZHnDp1qu+GKrGFIsAiIiIiIiJiCyqLKiJxx5UrV5yvZsiQIVAjkYBgmVD+FJE4ZsuWLYEeggTAlClTANy/fx9ApkyZLMZ+qXLlyjARYBaJGDNmTPLkyX0yUIk9FAEWERERERERW1AEWETijtDQUOerzz77bKBGIiIi3sWqzmI3s2bNclxmaQ/r2BqaGEMOCQlp2rSpt8YmsZQmwCISZyVIoEOciEjMxQULFo/Vd+/e9fFwJGZhK6PDhw87bkmfPr1bW8iUKVO4W7Zs2aIJsCgFWkRERERERGxB4REREU9cvnwZQFBQENw/J+3azp07//77bwB16tTx4mYtCpdGblG8ePFg/hriI3fu3NmzZw+Av/76C8BLL70EoEiRIgASJUoU2LGJWMSQ72+//QZg5syZO3fuBMDGNhH17NmTr3navXu3829Hjx4N4Ndffw33KB6Oli5d6s1xu2/btm0w0Uu+W0uWLBnYIcU64f7jMF3urIsYAeZLTmxOEWARERERERGxBUWARUTcduvWrWLFigFo3LgxgBEjRnhlsz///DOA5s2b8yp7P7Ro0cIrGw/nyZMnAH744QcAP/30E4Bjx44B+OeffzzYWsKECQF06NDh22+/9eYo45BNmzb17dsXwOrVq60/igF5/lUHDhx469atcHdgR5CQkJB8+fJ5bawiXsX2RTNnzgQwd+5cAFevXuWvsmXL5uKBRYoUSZUqlePq2bNnAVy6dIlXc+XKBaBSpUrhHhXYVJQHDx4AqFevXpo0aQAULlwYQMeOHQG8++67MAd2seLixYvhbnG3g1HEWpiO14/YmSLAIiIiIiIiYguKAIuIuG3AgAEPHz4E0L17dy9udsWKFc5Xly9fDh9EgB89egQTu16wYIEXt+nu6iybuHPnDoAmTZrMnj3b+qPCwsIANGvWDKYLSMKECQsVKgQgfvz4AA4cOADg6NGjAN54443jx49Di4ElhmGmydatWwEULVoUJhrsiAC71qhRI+erXL3JVz6A1157DUDXrl29OeJoa9++PYDGjRs7lxrmwXzq1KkwT6patWoBGmBsEjHnhUc/6yLeP+I2xYYUARYRERERERFbUARYRMQNLMA7ZswYnsvPkCGDFzfeu3dvACEhIffu3YOny3Gj1K1bNwCLFi0C8MknnwCoXbs2gCxZssDpfPnQoUMBcCT9+/d3vU3GflOmTOmLAcd2w4YNA1C/fv1y5cpZfxSTC+bMmQOgc+fOAD7//POMGTM67sCeqO3atQMwc+bMGTNmAGjVqpU3hy4SPVyO6xykTZ06NYD69esHbEw+w3frv//+CyBcp9kLFy44Lu/atQs+iAAzZ+SLL74AcO7cOe9u3KLEiRMD6NKlC6teR9/t27fD3cIS39ZFjADfuXOHiQlqW2BnmgCLiLjhu+++A1CpUiWmEHsX6xiNHDmSs5qsWbNafyxn5rly5XL9/WDevHlMxOWXMKbURur3338H0KFDBwAvvPCC9ZGIA09hsObNwYMHLT6K+aJjx44FsHDhQgDvvPNOxLuxuMv06dMB7N+/nz1XLE6A+f3v5MmTAJ5//nmLAxOJvsyZMwd6CL7CU13Tpk1zvpGFu9gMibw1OQyHE29miQdqAswlGGfPno05E2DnR/EcQVhYGJelpEiRItoDlNhKKdAiIiIiIiJiC4oAi4hYwvPr7OTBuJyP1KlThxFgi6G5wYMHA+jVqxeAbt26DR8+PNK78Zx3ly5dgoOD4TL2e+rUKQD79+8HULx4cXfHLw6sMVarVi24E234+OOPAXzzzTd4SuzXGbP4WrZsuWzZMusDY/rlypUrASxZsgRAjRo1rD9cxGMJEsTBb57bt2+HCTOGO7QyL5o5F0zHjdi3ySueeeYZAKtWrfLFxgOFfeCceZa3zNivw+PHjz0fk8QJigCLiIiIiIiILcTB83AiIr7AQFmePHngzin8jh07AsibNy+ATp06WXlIhgwZWFurTJkyVu6/Z88emNP/r7766tPuxk5FCxcuLFmypOsNsj4WAzUuAsU+wlP1HK1nj2XQgBF7d3ENm7eKo3D175AhQyzen1HcpEmTwlS3sihnzpwWK5Dxj8NGMpkyZYKC/CLRxohiv379Iv7KeUlw1apVAaRJk8ZvA4vtIh7WwsVyoxTp/bX6VxQBFhEREREREVtQBFhExJLffvsNQNu2bS3en1V5Wcu3WbNm1nd09+7dW7duAShRooSV+3ONGZf4uggDMuxgJfjACDDLeCZJksTSoL1k8+bNlStXhjfWaHk2cgbSWbE5V65cHu+d8WcGWi3+HwGMGjUKwIQJE+BmFPr8+fM5cuSwck+uQjx+/LjjMjuXiIjHIu1wxkr7PAiQupS5K1WqVOFucfejIeIq4qRJk8bJhejiFkWARURERERExBZ0CkRExJL169cD6N27t8X7r1692nG5VKlS1ne0d+9erv5lNDJKLD1qcQlolG7durVhwwYATZo08coG3VK0aFFGQR88eODBw0ePHg0gWbJkAFq3bu3BFrg2zGI01YWjR4/C/N8tNq48fvw4V/9ynblbNmzY0Lx5c+v3545ExHdYAoCYehNlUXcJJ+LnmrsfDREjxt76rJRYTRFgERERERERsQVFgEVEonDz5k0AZ86cAZAtWzaLjzpy5IjjcvXq1a3vLiQkJIBNWZctW8YKzAEpDpwsWbIOHTp4/PD58+cDSJcuHYCuXbt6bVjuO3ToEIDMmTNbf8ikSZPee+89d3d04cIFABs3bpwxY4a7jxURH/n3339nzZrluNq4cWOYIvNiXdq0acPdcvfuXbe2cPXq1Si3KTakCbCISBTOnz+PyKpxuHb27FmYVkbPPfeclYfwo3rcuHE7d+50cbfHjx+zTtWlS5dgWmvkzp3breE9DbcMd0o3SUQswXX48GHrD1m+fHnfvn3d3RHPF3z00UcJEyZ0cbcbN24EBwfD5GPXrl0b7r+kRcSikJCQGzduOK66tUIhOn799VcA586d88/uwmFFvTp16nhrklmsWLFwt/B8tHU8ReisaNGi0RqTxAlKgRYRERERERFbUARYRCQKFy9eBHDt2jUAT548QVQtahj73bt3L4APP/zQ+o6GDx8OIH/+/Pny5Yv0Dvfu3QNQu3btatWqAbhy5QqAAgUKAFi2bBmA1157zfruwmGxkGXLljFIWKRIEY83JfzrsQ8KXz+ZMmV62p337NkDIHv27G61buKrZffu3QCcky3DYSp+8+bNO3bsCGDmzJkAevXq5XhshgwZrO9URKyYOnUqL+TPnx9AyZIlw90hODg4a9ascLNEogv8dGjRogUANtILlCRJkjRt2tQrm+Jhkx3pTp06BeDvv/92awsRI8ClS5f2ytgkVlMEWERERERERGxBEWARkSgwNPrw4UMAv//+O4CyZcs+7c43b9586623YCLG69ats7KLkJAQACNHjgQwffr0p92tZcuWADp16lSrVi3HxocNGwZg4sSJiF4EmN2Pbt68yZAFmwmJZ3LmzAlTBKtZs2YAVqxY8bTEgQULFiCy1W4RMQHh66+/BjBgwACYbluRho7/+ecfAA0bNgSwaNEi9nY6fvw4gOXLl8MsF2zbtq37z09EIsfSDL/99huv8qAd0eDBgydNmuTF/fKIzc8ddytFeQurfFls4GddgwYNYD7pTpw4ERoaCiB+/PhWHnvixAnHZT6kXr163h2exEaKAIuIiIiIiIgtKAJsOydPngTQr18/mDUqrlcz+tqDBw9at24N4JtvvgGQMWNGL26ccZVu3bpF/NUrr7wCp1U60celno6Tvs4GDRoEwIMGJxJDOK/e7N+/P4DFixcnSBD++Hn9+nUAderUOXDgAAAG3LZs2QKz8LJJkyYRN75mzRqYMB1jgI0aNYp4tz/++ANm0S/Dv/hveNnFElOLHPWfA9IAKU76/vvvATAjoGrVquPHjwfw/PPPh7vbwoULYV5aLpw4cYI1n1euXAmAfY/KlSv3tPv/+OOPMAWf+WpE9F4zvXv3hslQ4Dq6yZMnQ8kC4g6m0jgwv8aif//918XVGIJJGaGhoSymEPGjnz3S4sePX7hwYa/vnUHOlClTen3LAcTvVyNGjABw+/Ztfmiy/UGUlixZ4rhcs2ZNRNWdbuDAgQAmTJgA0wqBXxRTpEjh6fAlJlIEWERERERERGxBEWAb2b9/P8wJMJ62D2zsl5IkScIARYUKFWAiqBabpkaJy2BOnTrF8Fry5Mkdv2LhXC/iqdywsDDHLQ8ePAAwY8aMO3fueHdf4mesyZwmTRqYxZNVqlRhnJY3spou43vXr19v3749TBz1gw8+APD+++/DRAZef/11nqTnq/3nn38GwHjyuHHj8JQ3Jt+/nTt35lW+0vhGpuinGCgC7HVvvvkmgE6dOgEYM2ZMoUKFYCL8b7/9NoBs2bIBOHjwICKU3eYBhAHb2bNnA5g1axbX+nLFOA/mLrBo6meffcarx44dg1npzdcta4lb9McffzA2QqdPnwaQPXt2mMiMiBXO3XEB3L592/pjucDVgUk3MQ3z7ADkyZMHAEs9O2MmheONKVFi1gzr2I8ePZrJL1FGgNmOYePGjQCeffZZAN99953rh+zZs4f/HTpz5gzMkdmDJu0SkykCLCIiIiIiIragCHB4PL/4v//9D8DRo0dh1qtkz5799ddfh2npFuv8/fffPFvG8qF8LjFE/fr1YYrrMiKxbds2AKlTp/bWLoYOHQpTl9VHuDbPGV9LPFUpsRorWzKKy1PI69evX79+faR3rlOnDu/DQwffcX/99ReAKVOmOH6Gw3KgEXtFOjRv3tz56ooVK2A6IvL89Msvv+zBUyOGlx2BCyvliMW60aNHAyhYsCBDwYzbO0fvqXHjxgzwsoos/7mMA9Orr746duxYWP4YChfuYJ1w1pHmizlx4sTWn8WjR48i3jh//nwoAizW8LXH14zD/fv3YQ5oLlISuG52+/btzjeyzAdDc84ZXoHFhaOIrMM2s4RSpUoF1SJ2Hzufb968mWUI3nnnHQB169aNeE+WiWZWFC+PGjUKFr4ERnqUi/RGie00Af7/njx5wmolPXr0gDkiR4plbPheSpcunb8G6Dl2wqhTpw6T8Z5WkT/gOEflxxtL3jPX1GKle19j9iBns/xLumiEI3ESX6JHjhyBeXE6sAjQ559/DqBnz57MZ06YMCFMpxme5eFjHZj+ymIbfFFZx8kMtWnTxv1n8x9//vmn43Lq1Kld1FUSj7Vt2/aNN96AKakybdo0mCxl2rlzp/P9edqFmdJMpOdlzzx+/Jh7JA9eM6VLl+YEnuMvWLAggAsXLgAIDQ2NIQdqiWkuX74MoHv37jCvcOejjQPz+atUqQLg3XffBcDqmCNGjFi7di3M+pFwFbP49mGBN3aAe/HFFwEMGTLEh08pKjyYt2jRYu7cuTA1R/ft2wczlQp3CkAs4iFx9erVXGTEb+PMJOerhSs7duzY8cUXX8CcNAkODoaZLUepZMmSXbp0gflc5mqgTz/91BdPRwJLKdAiIiIiIiJiC0HMSIkbHj9+zDRCJnclTZrUyqPu3bsH4O233+ZZRosY++XaehbIibFYNmD+/Pk8GebFvGJfOH/+PMyftGfPno6fnmHEo0WLFjxP7FkK9NWrVwHkzZsXpuQG//s8se2ikBhToNOlS8dmJKzjLz7F2JQjzYFn4pctW+b1HR05coTZClmyZIE5T+zizcV2HcztP3nyJGuwsXAaz2q75fLly4weM+zGdw33/uTJEw+K23F4bMZTqFAhBvdiI+Zl8B26ePHiQA/HFWY4M3jF9Sl169Zl8xJWlmLFLG+1MwkODmauINsXcckJsaAaW7a4i4n3O3bsiAklFUViFH5GsMQdv9WUKVMmwGOKQ3gQmzVrFswqHtY9zZIlC3NtGjduDBMWFglHEWARERERERGxhTi1BrhPnz6sN8O2OhYjwFw54Fb4FyYq2KpVKwCbNm2Cp6fPferAgQMAGH4cOnRoDI/9EuNpXPDGlhtNmzZlPCRQGDl3brfArgaKeNhZ3rx5mRRgEasNVapUyfEzOqZNm8a1ZOzv5fzWbt++PZfQc0WcW8Pj+XLxD+cAdZ06dQC0a9fOd7tzLBrn0dUZa55t377drZpYxNXvOhiKRFSqVCnHT/E6htMVVBfPxLg5m4iIiIiIiIgvxJEI8MqVK2FqtFrHFhG//PILgHjx4vEsOFfWsX32H3/8AVO4P1Jbt26F6V8S8bR6wHXu3BkmtsNgdWzBmnvsHdK1a1eWUgyU8uXLA/jyyy9hlu199dVXARyPuMDVjHHekiVLeMG5kcbt27cBrF+/PlzzG1vJnTs3Ius+EgOxNikVLVrUdzviAu9Vq1YxTstoM61ZswZAnjx54GZLJJjPx8Cm54iIiHhAEWARERERERGxhVgfAT5x4gSARo0awZ3gD8vz9u7dG6aI5bRp01566aVwd9u7dy9MIdmLFy8+bWvz5s1DDIsAr1q1yvGTxYdjxQJgB0YV+G+dMWMGG+ixZK7/cXX3gAEDArJ3cQuLQDp4UGA5Vnj48CEvODejZt5Enz59PFjJGWew8HsMrMjg7MaNGwDWrVsHM1SfHtzYOjU0NJSr1lkTle8UfghOnjzZg82y+CoXnIuIiMQisXgCfOXKFQBvv/02/lugyIqxY8fC1Fti+nSqVKki3q1IkSIwX1PKly/PaXNER44ccWvvfsAW3sQ2SLHRxx9/DGDGjBks38KMaBEXdu/e7Xw1V65cgRqJTw0aNIgtbZiZzyUbbKVj80JWMXzqS0uXLoWZlzL9mKWkfIQb//rrr/v37w/g888/h6ndOGjQIJgeb9bx9bZhwwYAw4cP9/Z4RUREfCsWfFcQERERERERib5YGQG+cOECgNdffx2mRY11oaGhMA0hfv75Zzwl9uuMZ8fbtm3LHksR/fPPP26NwacYDA8JCYFpblGwYMEAj8lTJUqUAJAyZcoZM2YAGDZsGIAkSZIEeFgSg7EunUPFihUDNRKfeu21186cOQPg4MGDAJ577jkAGTNmDPCwxBq/lb9y9tlnnzVp0gQAXzlMfn722Wfd2gg/QNu0aQOzKkQNkEREJNZRBFhERERERERsIZZFgHnqmrHf48ePP+1uJ0+exH/L4aRMmRJAunTp4seP77gDL1v01ltvPS0CnCNHjog3sjqXdZkyZYJZrMWRX7p06Wl3zpw5M4CkSZNG/BUrcrHvRWzvD87lfOXLl//f//4HYNmyZfhvDw8RB77jjh49yqspUqQAUL169UCOyZf4BGP7e9xueGResWKF4xbm6fgHy17wp2fYAe6VV14BUK1aNW8NTERExJ8UARYRERERERFbiE0R4LCwsEqVKgE4ffq063s6twYhVkIeM2YMr7oV+6X06dM/7VcsFh3OW2+9BeDYsWNWNp4zZ87p06fDLFnctm0bgI8++ghOES1ig5O1a9cisqfp+BWVLl3ayt6tY6OpJ0+eePBYD/7mVKFCBUaA169fDz9GgPlfOHz4MPtjlSxZ0j/7Fc/069cPTi9OlhCPNEtCJFC4YjZdunQwnynvv/9+gMfkju7du8P9lcMiIiIxiiLAIiIiIiIiYguxKQIcFBTUsmVLAPfv34epCRypbt264b9nqUuVKhXNvbsoN92iRYuINzJy265dOwA//fTT0x7LJVV9+vRxvpGLnNleeOTIkQC6detWvHhxAKtWrQKQOnXqp22QvRkp+s+az6JPnz4bN26Eqb/tWQQ4Q4YMMGFVt7qzOgr5cgw+9eDBAwD16tUDkCZNGgCFCxdm+sC7774LYMqUKb4eg7jrzz//BDBr1izHLTly5OBBQCRGSZQoEcxxlTUOEiSITZ/Civ2KiEgcoAiwiIiIiIiI2EJsOvccFBTUt29fANeuXYOFCLB3u2Lu3bs34o0MsbroNcp1idOmTYOJLoZz69Yt1/vlol8AQ4cOhcvYL0tkM0ibPHlymA7GnuFaYi64jXKQVnDxG0vXusVRJXXPnj0wXZefeeaZ6A8povbt2wNo3LgxgKZNm/LG5cuXA5g6dSqARo0aQeVPYwx2vW7QoAHMAnXG06ZPn87C7yIxEOPAIiIiEhCxaQIcKMz4DQ4Odr6R32AmT57s+rFsbsQc6R9//DHiHWbOnAlgyJAhCRMmjHQL69atA5A/f/4qVaq43pdzXyh2ZmKKnbv++usvAHXr1gWQM2dOAJ07dy5RogTMtJNT2Tt37gB4+eWXAWzevJllXZ4mUaJE7L3hQSmsZMmS8SwAO4iwBFq+fPnc3Y5rc+bMcezCMfUlnlOgXbt2IYZNgCdNmgRTIcyz7PTo4GuMr5Z33nnHn7u+f/9+zZo1ARw8eNAxEhaTY7U8EREREZFwlAItIiIiIiIitqAIcNRCQkIA7Nu3z/nGUaNGAShQoICVLXTp0gXA+PHjmaXp7NKlSwCWLFkSsbvPjRs3ACxduhTAkCFDotzLyZMnHZc9y/9k/LN+/foAmjRpAvM0Iy3TsnDhQph+Hl7vtxQO61ExEsun6fUIMDPqmazu7OzZs4cPH3ZcZUukGIWvzJ07dyJwEeD8+fPDjxHgU6dOAWjatOnWrVthCvMwHYOvXhERERGRSCkCLCIiIiIiIragCLArjKf179/f+cYBAwbAVEuy6MUXXwRQo0aNxYsXR3qHSZMmRYwAczUjF9w2a9Ysyr2cP3/ecTlVqlTWh+fw7bffAsiaNSuA0aNHw+Uq4kWLFgHg2mBfc44Anzt3zrsb3759O8wzLVSoULjfzpkzhy8Drl6OgYtLv/vuu0APwX9Yiuzjjz8GcOfOnYIFCwKYO3cuTBRaRERERMQFRYBFRERERETEFhQBdmXcuHEAdu/ezas9evQA8OWXX3q2tY4dOz4tArx8+fKzZ88CyJYtm+PGCRMmwLR4cdH9yOH+/fuOy56tAS5TpgyAzp07w2XslyuZuTi5Xbt2Hrui51sAACAASURBVOzIXYwAk/PT9IrHjx/DNKyKyLEquGrVquFGIv7UunVrmIW+adOmBTBmzBi+/CJdoy4iIiIiEpEiwCIiIiIiImILipxE7tixYwC6d+/uuGXAgAEex36patWqL7zwAoATJ06E+1VoaCgXN3IXrG174MABmDiwFc6h0RQpUngwwsqVK1u525YtWwBcvXoVQPHixT3YkbucA+BejwCXK1cu0tvZ8pf/BQCtWrXy7n7FLY5/BIBixYoBqFGjhmK/IiIiIuIWRYBFRERERETEFhQ/Ce+ff/6BWXnLy+yF+8knn0Rzy0FBQR9++CH+G1h24OLGXr16wUR9CxcuDLMu14qECRM6Lj98+DCao3WB9Z/JPxFgNigm56fpU1OmTOEFrvv1W5NbidSKFSsAdOvWDcCkSZMAFChQoGfPnjBvqMSJEwd0gCIiIiISC2gCHF7btm0B7Nu3D8D48eMBtGnTxsoDBw4c+NxzzwFo0qTJ0+7TokULmDzncHPUkydPAvj1119h2roMHz7crZEnTZrUcfnWrVtuPdYtnABzWpgzZ07f7cjB+ekkS5bM17vjfHvWrFm82rhxYwCJEiXy9X7FBXb2mjhxIoBGjRoBaNCgQZ8+fQDMnDkTwG+//QYgR44cgRyliIiIiMRsSoEWERERERERW1AE+P8MGjQIwJw5cwDMmDEDJvQXpStXrgAYNmwYszRdSJ8+PYB69eoBmD17dsQ7sNcLuYgkR8q5Q4+PIsBHjx4FcOTIEZi2QP5x8+ZNx2VGAn0qJCQEwI0bN3i1efPmvt5jdGzfvh3Azp07ATx58sTPe48fPz6ASpUqAXjppZf8s9MqVaoA2Lhx45tvvgnzgmQJt7Vr18JfiQkiIiIiEusoAiwiIiIiIiK2oAjw/zd9+vSBAwcCWLBgAYBatWq5vj+XibIh0KeffgrgwYMH7M4SpXbt2uEpEWBGHbnq2N1WRrly5XJc9lEE2P/lr8j56XChtU+xJRXlz5+/ZMmS4e4QHBwMIGvWrABKlSrl6/G49sUXXwBYvXp1AMfAzAUu0PWbAgUKbNiwAUDRokVhVtG/8sorANatW+f8dhARERERIUWARURERERExBbiZgQ44krIR48e4SkddFauXAmgdevWfJSLBZ+8Q2hoKIB79+6F21GJEiWSJEliZXhcMJk/f34Ahw4dingHhojd5RzyOnfunAdbiFJAIsBhYWHXr193XPVpZO/SpUsw9YSpZcuWEe82ePBgmGY8Abdq1SoAt2/fRiDWAMeLFw9A8uTJ/bxfYjrA999/D6BZs2YATp8+DeD9999ncDgoKCggAxMRERGRmEkRYBEREREREbGFWBkBTpAgimGzLHOmTJkct3BNb7NmzZwLO+/YsQNA3bp1YULE+G/BYbe4uxaUYd5PPvnE+cYSJUo4frorX758MO1qL1y4AODixYv479/BY1evXoVZ80x+iwAfPHiQK64zZszo+OkjXEnLID9jm++9957zHRixZ+njwoUL+24k7nJ3xXhc8v777wNYuHAhTCftTZs2cSF3pAF8iYgrqPv16wdg6tSpipyLTJgwAeYjNYb3AhAREbcoAiwiIiIiIiK2ECsjwClTpgSQIUMGAJcvX454hwEDBgBo3749TEXfv/76C6YBL0wcr3r16gDu3r3rlVGVLl3arftzyWKPHj0A/PPPP7zRs9W/xBXILFm8efNmALt27QJQo0YNj7fp8L///Q8mNMpg4wsvvBD9zVrBPrcwa6d9inEwypMnD0ypZ4fevXsD+Oyzz3w9EnEX3zuMAAPo378/FAG2YP/+/QBq1qwJYPLkydDCaREAQNOmTWEKy58+fbpPnz6BHpGIiHhHrJwAE5OZv/vuu4i/mjdvnuNnsmTJYFJbEyVKxBo5b7zxBkxar7e4mwKdKlUqAA0bNgQwdepUlhFyztD2TMWKFeGDCXBISIjjMrvO+O1bsmMCzKfmU87J5zzD4jB+/HiY/5rjTIrEHK+99hqA1KlTA7hx48apU6cAbNu2De6fnPIW1iTbvXs3gKNHjwK4du2adzfO9SBZsmQBULVqVQDPPPOMxS38/fffjkd9/fXXAF5//XVvDU8ktuNbiV8kihcvniZNGgAdO3YM8LDiCseCIx6CovON4uHDhzCL2nhYYzwgf/78efPm9cpoRWTLli3r16+HWV/Jn5xJcXHirFmzAjpA9ygFWkRERERERGwhFkeA2YrmzJkzAIKDgyPeIXfu3DC9ahwhICZGnj171osjYUowa1C5i3mbU6dOZSEfxqujg6mMQ4YMAbBz585obs3hzz//DLcLv9m+fTvPDbOSmU+9+eabAFq0aAFg7ty5APr167dv3z6YMmnz58/39RjEM2xyxhfJtGnTeCN7RPk5Avzw4cPhw4cD+PHHH+Hto40L7Kx24MAB1m9zgWsu6tSpA/OaV664SKRy5swJYPr06bVr14Z5lylXIjr69u0L82UMJqg+ZswYd7fz+PFjfg9k3In/KWb93Lp1i/dhBjs/zcNldQUQg2affvppq1atYNKXxEfYJHLcuHEwL5Lo51ra05IlS5i4wVxa9g0lP88LvEIRYBEREREREbGFWBwBTpo0KUzvE8bo9u7dy8gGY78888eONQ4MCPNnTMDY1KBBg+rXr++VDZYvXx7mLPWaNWsA3L9/H+bP5TFGPvfs2QOgUaNG0R+nFQyd7du3r0qVKjCn7vxgypQpMEXUDh48yBBZmTJl/LN3iY6XX34ZThHg48eP+3Pv9+7dA1CuXLkjR47ABFfLlSsHs1iXYeqgoKDr168DWLRokeOxfHm/+uqrT9s447qPHz8GcO7cOfYkc9TPA8BuYVwb7BqruPH9tXLlSrefp4jNVK9e/eOPP4ZpjMc3OEtCiLucj3swFUbcigCHhYUBqF27NkuE7N27F0DmzJkBnDt3DqYmy/nz57lkkdUrf/rpJy89A08wiYzPnf0vz507V7ZsWdggArxp0yaYEpU9e/YEkDZtWr/tncHJjRs3Om4pWLBgoUKF/DYAX2COA+tWMrWhSZMmvt4pEy5g6go7l8KtXLmyr/fudYoAi4iIiIiIiC3E4giws8KFCzt+xkZffPGFdzfYpk0bAF27dgWwdOlSANGMMBcsWNDx029++OEHAKGhoVwn42c8hexucW8JrGeffdb5KgOtfsPTsS+++CJD0KyXHqkFCxbgv5GQDz74AECvXr0s7os13hnxfuutt2CWHIfLeYnowIEDvOfQoUNh6maLiGvdu3cHMHbsWJhVrJE2oZAoMTWGCWXwqFEF13MuXbq0Q4cOMLFfYudCBhsrVqzIvJjs2bN7YdyWsdp/cHAw+3Sy7+by5cthcnaYKxS3sXcm+wswAsxAJf9l/okAM+rO8uDE0u7+jD9716VLl0aNGgXzFuDyZqZ0+SEC7ODcMZSYchu7KAIsIiIiIiIithBHIsASDusYDxw4EKaBrbfWGPsHz9pOmDABQO7cuevWrRvoEUnsEK6ZJPtD+k2nTp0cP13jyjRn7taVLV68OIAbN27AzbWInTt3Tpw4MYCAJFaIj8ybNy9lypQwXe7F6xi/at68OUwcuHXr1ojNqWeBwpWEFStWBPDkyZPq1au7u4Xvv/+eF55//vlI71CyZEkAO3bsYKMQxpz95sqVKwCmTp2aPn16AAUKFICpRM36zw0aNPDnePwjNDQU5mkOGTKEpXmyZcsG4Ntvv4XJcop+rxPrWHRj4sSJMPlWH330EYAsWbL4bQzRx3DriBEjAEyePJnfatjEoUePHjCJYP60bt06x+XkyZMDKFasmJ/HEH2aAMdNadKkATBgwACYHgM7d+70/5vEY7/88gvMp8jEiRMTJUoU6BGJeJPzBJh91PiNzTpO9d2a+rIp1KpVqz788EMo+TluGT16dLp06aAJsI9xYRFPzvbr1w+mEqdYx2OXB/NemG/eLEKGqGZThQoVCkitIzbFZN5vOFz8EmcwVjF16lQAw4YNgymPlDdvXtaaZYNPTkQDhbnB/swQjr4DBw7ALFOaM2cOzLumadOmn3/+OYC8efMGcHhr1651XK5QoQIsrL2KgZQCLSIiIiIiIragCHBc1q5dO5hz1b169VqxYkWgRxQ1ZtF88803MEmhtWrVCvCYRLzq+vXr+/fvd1xl6yM/nEDloQAmK0TiksePH7NFlvjUiy++CJNSu3jxYgCXLl3KmDFjgIdlG2z/JoF1584dVlLkV7WLFy/CJOIyDlynTh327RO3bN26lWXDlixZAlOyi2XDunXrBpNSHkDsq8r2YxQbGyCRXqAiIiIiIiJiC4oAx2WMKTHsU7FixVmzZgF47733Ajwsl1gs4fTp0wDmzZsX6OGIeN+GDRuePHniuFqlShVf75HtoEJCQgAUK1bMz/3MROIYZiex0cu0adPYIUn8gEWtxP9Yk4Wtv8aOHXvz5k2Y9KWff/4ZQNWqVQM6wNiKuZkM/K5fv54VfHr37g3g448/Rgxr2rR582aY/lIUGxsgkSLAIiIiIiIiYguKAMd9rC47fvz4Tz75BEDZsmUBPPfccwEeVgSsevfVV1/BVNLnaiuROGblypXOV2vWrOnrPTKZghU7y5Qp4+vdicRtbOFDM2fOVAQ4Orh2PUECS19H79696+PhyP9hvJ0NeNhM6MGDBwBq1arVs2dPAKVLlw7oAGOlsLCw+fPnw0R99+zZA9OZacSIEezR8OyzzwZ0jE/l3ACJNdhLlCgRuOFEiyLAIiIiIiIiYguKANtFs2bNjh07BqBGjRowpRTdaiLqO5cuXYKJgw0ePBjAW2+95d1dHDx4EABXrRCbdz///PPe2sWpU6cA3Lp1y3HLjRs3vLVxiUsWLVrEC0WLFgWQK1cuX+/RuWtf9M/ZHz9+PHv27AASJ04czU0RV0QvXboUQLp06QISo2b9eXex0ik7NIqPXLhwAcCePXu4lJ3FZvPkyYPA/eWLFSsGEwDZv38/B8bFexIlhnx/++03ADNnzty5cyecWvuG07NnT4bIaPfu3c6/HT16NIBff/013KP4xuQhRdx18OBBFnOeOXOm40aWj2ET2pdeeilQY7Pi+PHjv//+OwCWZ2eZ4sD2In748CHMYulhw4bx23ju3LkBjB8/HkDz5s0BJEqUKICDtMI5Aly+fHm4zN0ICwuDaYh9/PhxAPny5StXrpzPR2mNIsAiIiIiIiJiC4oA28iAAQMApEiRAsC7774LsxYxsOGLBw8e1KlTB0Dfvn0BtGzZ0hd7qV69erhb2Mhx2bJl3tpF586dEdmpaBGHXbt2ATh79iyv1q5d2z/73bBhg+NyqVKlPN4OT6tXqFCBr3NvrV5u06YNgMmTJwNIkCAB8zV8VAKA0eYffvgBwE8//QSAJ+P/+ecfD7bGqAL7NLKCvYTDP8vFixeHDh1q/VHXrl0D8OWXX8I0MggXoudrjxEqfqj5E4MeTFVYvXo1K6O+/fbbfh5GLMKkM/6/WOPj6tWr/JXr1qZFihRxTlXjwZNZYzDpM5UqVQr3KCVluGvbtm0AhgwZAiAkJCRJkiQA2rVrB9OBNmfOnAEdoCX8Grlx40Z+5eMiW4ZVmQblz7RHrldngHfkyJEAzp8/D6BYsWK//PILzPfwWNQw+d69ezt27HBcddEBeP369QA+/fRTmEXChQsXBjBw4MD8+fPD/EECSxNg2/nss88ANGzYEDHjQyJJkiQ8FjCp0rvYrIItxcNJnz69d/fVq1cvmK/y4RQqVMi7+5JYKtz5ET9MgFnFhEmkTPvPmzevB9vhvJFdGSpUqMCVFN6yfPlyx+XHjx+vXr0avpkAP3r0qHHjxgAWLFjgrQ0CSJcunVe2FscwtfWbb74BsG/fPouP4ryI7VVYGZGv23z58jHT+MSJEzBH9bZt2wKYM2eOD4YfNSZCr169euvWrdAE+Cl46OCfiIs+OBl2TIBda9SokfNVvqj4wgDw2muvAejatas3R2wbq1atgpklrlmzBmZ+2LNnT05dvP41yXemTp0K4OLFiwBWr17NL7c8rdmnTx8APAHHJ+tT165dY2b+mDFjYJbC8RzNpEmTYKIvsdHmzZujbIDE584WcTxKZ82aFcAHH3wAYOXKlWz7xHq3ga31FWtOPIiIiIiIiIhEhyLANpUjR45AD+H/+CL2S8ytcp1h5S0szSLiQnBwMC/wxOehQ4cAHD582PoWWMLKet0sVp4gvus9S7hi7iIDL/v37/du8gjPzb///vu86lk2shXdunVjBTL2hGMEnv0n4seP7zyYe/fuAejfv7/rDTL2mzJlSh8NOFbjqpBx48YBSJ06dZT3Z5Izc5uZucDHtmjRAsAzzzzDu/31118w2YPs7zVw4ECWk/GztGnTOg9JIsVjhXOQli+G+vXrB2xM9saCnQ0bNty+fTuATJkywRz32rdvD5NzEVvw6bBXExN0HR9PTH0irj/yKY6hb9++/AhjnhT7RcWcyk/R4aimmTRpUpgeq86GDBnCanaM/XKpCItgsY0WzLohi23PfEoRYBEREREREbGFwE/BRUTivL179wL4888/eZXlMbge1SJW8nAuQWHFyZMnHZejE6jkuW0udH/hhRc83k6kmjZtCtOzZM6cOVwy5F0MFc6ePZtxABfL8lnoi3WtvP5MbYJFoZhrYH1lLMuinDt3DmapZ6TrwNm7bvHixTB9RHbs2BGQCLCj75Hzu0yilDlz5kAPwdYYF2X+EYCyZcsCqFatGmJb7Je46PTDDz+EU54Isc4r+aF1Ez877t+/nyFDBph2nsWLF/f1fv3G0QOJrxnnzlJcXP3nn3+y1ZNzotlzzz0Hky9z/fp1luNllbXAUgRYREREREREbEERYBERn+NpUQoKCmKThihX0rJcKkNebCfDXgLWse8CedYBgrE4rrPq1KmTB1uwiO3Q5syZw+drEZdfclF0pMub79y5A6BLly4AgoODXZdkP3Xq1P79+xG3Ttv735QpU2Ci6BZdvnx58ODBMC27oqwBzjQBhq344rSCFYkZsHXrZRYpRwSYUWuxKCYs/7Mzhu9OnjzJ2On3338PU6CbH0w9evSoWLFiQMdoCSsSz5gxA04BbeLHlnMJDBaW96n58+cDWLlyJbtJ8QDIcsesqv3RRx8hdtaMYM4a/6r4b/1nZofxc3P69OkRP4V5rOZB8vr16zEnAUQRYBEREREREbEFnYcTEfEh1radNWuW45bq1atH2ps6osePHyN6AZP79+87Lnt24nnatGkwJ7NTpEgR5f3ZrLV58+YAevToAaB8+fJWdsTYbJIkSYoUKWLl/gwYcllyt27dAAwfPjzi3RglWLhwISKrWhnOokWL+NcOSO/usLAw5y6L7j6Wr7R///3Xsy0kSpQo+sW9+Xr77bffAPzwww/WHzhy5Eh2mXYrxyFnzpxw54XNiDFXBvI9GJ2O1o4IsPO7TCRWSJs2LYOTn332GYDx48fDdO2uVKkSqxazgjHfI96t/O8VDEty/I6S7MSPLWLqk9+671atWrVq1aoAWGSb0WB+TvFy+/btGRBm/e1YgZWc+YUEpukJvxUwrWb69OlwaqYQUeLEiRHD1v8rAiwiIiIiIiK2oAiwiIgPLV++HMDFixcdt3Ts2NHiY6O/WM45NmUlfhsRo3krVqyweP8JEybAhNfq1q0LyxFgrswsUaKEc21JF/bs2QNT9tPF+i6G6RzBOtcWLVrEYqF+rlHJssmVK1d2nGL3mMcjf+aZZw4ePAh3ukxHxFXrjA9Y/D8yjLNmzRoGGdzCJe6Mt7jG8DgbWTPwEv1l3o7mxooAS+zFpvSsksDPphkzZgwbNgymhHvBggVh0nkaNmyIGLOKm29ARoCdPXz4cPbs2Y6r7733HkwE0p9KlSoFk3x0+PBhmGbL33zzzahRowC0bNkSZvzRL0ngU476z8TGDW3atAEwaNAguIz9xlgx4kUsIhJXjR071nE5b968MHmY/uE8CXn48KFbj+WkPSwsDO5MilavXu24zG8AFu3evRtAlSpVLN5/zpw5MDWuol9W5NatWwA2bNjQpEmTaG7KA6xAM2rUqAcPHni2hdGjRydLlgxA69atPdtCihQpcuTI4dljHViKxuIpD+L/sWnTpm59Q+WElkVZmL3pGr+fsSgOL0f/C7Ej29ziVF8khmOzvVatWrVo0QJAcHAwTOIu5zxffvklgM8++6xVq1aIGc1sIlq0aNG1a9ccVznUwMqXLx9MdcD+/fuz5RtPFvNngwYNAPTo0cPdOpf+sXbtWl7gCWIuJpo5cyaAMWPGAHjttdcAdOrUyZ9fb6JJKdAiIiIiIiJiC4oAi4j4xIkTJ2BSoIllfvxZTSRp0qSOywxyWvf777/DQkOacI4cOQITMS5QoID1By5atAimJ4cVbLfgrZYSy5YtA/Do0aOANEBi8NatvkHhzJ8/P126dAC6du3qtWG5jxHgChUqWH8Iy9XMmzfPrR39+uuvMLXKHKnIUXJ+O0Sf4w3Ff59InMGja7169Rw/V61aBeDrr78G0KFDh5jc3YeBVpgPoBIlSgR0OOFlz56dKdC9e/eGiaDy5+zZs9mJiuXH3DqQ+ghzrHbt2sWrzZo1A/D555/DZIfNnTsXwAcffABg6dKlzOhmCn0MpwiwiIiIiIiI2IIiwCIiPjFu3DiYJgHsxMDmQP7kXPzJ3QgwKwxx5BaFhYXxUexLZBGjr3///TdM/ST/Y/wZMS9cELtwTSArvkTpzJkzMFFZ6x1BGJFgoJu9NwLl5s2bvODWe0QkNmJ1Bv7csWMHFwZzSbCjuw9MTDhQDX4uXLgAp5KN/v/AdQtbN/Xr1w/mE3PChAnsRFWxYkWYYgqMBjMy7P9mVBs3boSpuQDglVdecfyKaQKNGjWC6ZD0/vvvsxkhswZKly7t59G6RRFgERERERERsQVFgEVEvOz27dsAJk6c6LiFyzv9v1bQuXqzuxFgVoG+evWq9YesWLGCZ4JZE9IiLidj+WUXZ7i55UWLFl26dAmm+U3u3Lmt78jFZhmFjhcvXpEiRaK5QTvjX48LuRnZYJQgUmwQYr1qqCPIAKBMmTIwcRIXbty4ASA4OJjDqF27NrwXsHW8oZ577jmvbFAkVihZsuSCBQtgcj244JOhy++++w5AixYtAtLdhykhoaGhrPTOytXOjh49CmDLli0scx1zsBlV586d+VVhxowZMH/YmjVrwtQ76NGjBztR+a3tkHMDpGTJkj0tReudd94BEBQUxJS39evXI7II8Lp163gYt97uwXcUARYRERERERFbUARYRMTL2JiUcWAucfzkk08CMhLnCPC5c+fceizP1O7YsQPAP//8A+CZZ5552p3ZxZcnp2FOGzNI6wIXPvGx7AcbqXv37sGE76pVq3blyhWYCp+M3LoVcA5nw4YNMOs58+fPr4q+0cGQLCuCDh48GGaVYKQYROrfv7+VLd+9e5cFSPfv3w9g27Ztru/PauRcBNixY0e2rOzVqxfM6y1DhgxW9usCX4dwp0u2SFzC9raTJ0+GSeRhHHjChAnh2tsC8EOHW0dRAOaVZM6cOdwdOKrs2bP7eiQec7RiBhCxG3OTJk0crZgBtGzZEj7uxuwcAS5fvnyCBJFPG9nBnuFfPL3e/rhx4/jpEBNoAiwi4oYDBw4AWLBgAee0EdMpHz58yDQw4uE+ffr0fhzj/+EXFH6mskDIxYsXLVYo4d04M+TXGs4fwuEfhCU64sWLxz8I/wKcseTNmzfiozg7GjlyJExBIxezCH7Md+rUCUCtWrX4kcz0MOaZR2cC7Ch/BSAgPZDiEmY8dunSBUCfPn0AMF/966+/Zo4f8cYtW7bAwtdi/rs/+OCD69evA9i0aRMA9nyKFE/W8FwM/7k5cuQ4fvw4TE8ytlBq27at588TgJlIW3kKzvh1ln1EeFiYPHlypO+RuOrhw4fOV3mizaJ///3XxVUJIM4qv/32WwBffvlluO4+MFmyU6dOhQ/qxrH4Ij+MYBZKOOPLLCQkBKbDX8wXaTMqzoTZeoonHUaPHg1zrsGLeAb/jz/+cNzi4nOWNSwd+MXDGWfIu3fvrly5suv9Ll26FOZDhL21eIalYMGC7gw/akqBFhEREREREVtQBFhExBJ2Ixg6dCiAJ0+ebN68GSYF17kixZQpU86ePQtzkjvSqKnfMDmqZMmSADjgXbt21ahRw8pjy5Ur57jMtKtjx44BeOONNxjDYSxu2rRpMOfXf/nlF8bcmHHK/X788ccwwdUbN27MmjULwJo1a2CqB7nIkuXpZ+aa1qpVizc6J2VFv9+GIsBex9gv/00siBUcHPzee+8BePvttwHs3bsXJkGRvUAcrl27BhOqZb4i66nkyZOHcZsXX3zR9d5//PFHmIT5HDly8EbvvmZo+/btvODcGsSFsLAwmHQGFtDiG6pJkyY7d+70ypBiBVYmc2CgySK+PByYFBCLcDWHA7MV4p60adOyBh4zdbkmiIWd+B/0egT45MmTzldfffXVcHfgYYGp0c7dAWOXKlWqODpRweRF87LXI8DhGiAhsr+qQ7iDasTilPye0KhRo4QJEz5tIzxCMnrvfJRg8tfatWvdGX7UFAEWERERERERW1AEWEQkCly+wlPIjjIPK1euhInn8EQsz2oPGjSIhSImTZqEwK3+dcbSRO5GgNnwgI1tGLLjSVz+DGf48OEAGjRowNWM8+bNA3Dnzh0AgwYNinj/5MmTw0RfXUQDWPGoc+fOjlvCwsK4IogYV/QMN+4cOihWrJjHWxMHVkBhdTGevB8/fjxfIfxJrKnGF+ejR4/YdotryRgK4BI4LtYdMmRI6tSprez91KlTMKEnOnbsGAfDyI/1xktP4zzUtGnT5s+f38qjeOiIuOR1165dHHOcL6bFv8D8+fOdb7x//z6AFStWwOW/5tChQ3CKuhPrqPXt2xfmkBJzMu9pjQAACRhJREFU/PXXX4ye8eODVdlGjBjhfB9+pmTLlg2mqh9zduLFi8fsGBddxGIF1hTkAdz5MO51L730Esxf79GjR855JQyQzp07FyZjKw5gdhVf/z7iHHHlWtwSJUo87c5ZsmQB8PLLLzOZha954tGSZUH4relpnnaEfPTokfvDj1rsfmuJiIiIiIiIWKQIsIjEHSx37OC8fCU6eF6Zq38//PBD51+x7wJ/8mx34sSJFy5cCLPcMSaoWbMmTJja3dWGLJvJyo2MUzlwgSUrOb/77ru8kRFjLvdiBWwud3Tg0uIpU6YAyJMnj+u9s42NsxUrVjDyxh0xTO2ZP//803GZ0UXnZc8STXwz/vTTTwBatWrF/zibXfElwQWQXEnuwH9EnTp1YJqHuds9xbkGO02cOJGxBa4uS5w4sUdP6P84xyFr164dFBRk5VGsFPDDDz/AVD7nESNjxoynT59GHI0AX758GUD37t1hDj7O7zsHHqO4vpEHk9atWwMYMWIEI1GrV69GhOgQj0g8ELFELZeI81gXWEWLFuV/nIseeZkR3axZs/I+fAswNMpPKwa77t+/z7LG4VbIy9PwuMF6yB06dGDiCf+kLCTBCLC63FnnvMCeJRWca51EaurUqXwjMzPujTfegKl7z69Pjld+pLh9rhjnwT9FihQwTSi8ThFgERERERERsYUgx3o2EZHYjlEChhFg1smEWzYWTaxv3KtXLwZtiOcp2ba+Z8+e3ioz611cJXXmzBnGZJ7Wqj5SDLzs2rULwJEjRxi55YpZF/E0Fvxk8V7Wei1evDgfazFoFlG9evUYYGeHyY4dO3q2HZgmotxaoUKF4INOg35TtmxZtsZdvHhxoMfiClMDGInl/45LHzNkyMA4HgP7LiqFuoWv22zZsrHzMFd9R/+/3K5dO5jg9ubNm6OZONC+fftWrVrBHK9EJDpOnDixdetWmG7hTA0Ilx0mUeLaXRbhZwsGVm1w7e7duzAVpJngwO9j/I4UoygCLCIiIiIiIragCLCIxB3nzp2DKaoJc9L35s2bsHby0rqwsDA2++WSrVhRsZMLabp27crVUPXr1w/0iNzDwHW2bNm4Uohr5Lj0ix9kHkeV44DYEgFms0oWCT948CAAiyWUPcPlZ3Xr1i1dujRMMgI5F5q2jgcTHmH48/Dhw9EcZM2aNWfPno2YV8dYRCSuUhEsEYk7WGLhpZde4nfrhw8fwlTzr169uhd3FC9ePGZsxiLM0B44cCCLTMS6CTDbLz169Khhw4YwU19q3749gAYNGjDbzYZy586dIUOGQI/CFSacsw0Jz0ZFWQIt+iZOnMgLrMfmjAn8XB9hvSwWS3ndu3cPwOeffx7N4fH02ZkzZzT1FRHxpxgdrxARERERERHxFkWARSSuadOmTZcuXRxXmfrr3QhwbJQmTRoAAwYMYPEhtiSJThshP1uyZAkv1KtXz3Eja2uxUEfE/jf2MW3atBiegb9y5UqYEimlSpWChaYa0cGA86pVqwAEBQWxrxKtWbMGJv5sPfbLNPuxY8cCKFGiBCLr0eWuFStWwKlon4gfMO9gwYIFTGTwG7Z0YkOdOIk19nbu3MnlFX6QIEECABUrVgTw/PPP+2encUaM/rwUERERERER8RYVwRKRuObevXsM77BOEk2fPh1A06ZNAzasmCE0NJTxq4wZM8LEoGKFsmXLAvj999/ZnoHjZ/+YqlWrAmjcuHFAByiutGnTBsCkSZMAtG3bFqaNkI8wupUyZUoAL7744qFDh2Diz9WqVQMwefJkAHnz5rW4wQULFgB49913YZp8VKhQwePhMUbEl/SCBQscdftEfI35MpUrV/bzfpmicvz4cZiykXHMq6++CmDdunV+3i+re7A8gVinCLCIiIiIiIjYgiLAIhIHccEhQz08yiVJkgTm5DeXINrWjh07YBYOMQ723nvvBXhMFnDpZt26dVm/mm2oGPt1XvItMVBYWFimTJkAXLlyBcC4ceNganf71PDhwwH079//o48+ArBp0yYAgwYNgpsRsFu3bhUoUADAO++8A7MSODq++uormDL1HI+IP924ceP+/fv+3CNLnadIkcKfO/UnLq6+cuWK3yZWLKOQPn16+LikQpykCLCIiIiIiIjYgiLAIhJnTZ06FWbxYWhoKIAsWbIAWLt2rR96kMZwP//8M4CuXbvCdEONFeuybt++zSbPHC1XAksMt2nTJmYc0NatWwGUKVPGP3s/f/78mTNnABQsWBDAs88+6+4WmjVrdvr0aZiy0gkTJvR4MKtXr4YpTb948WKYtZEiIuI3OuyKiIiIiIiILSgCLCJxXEhICICGDRvCdAdNnjw5K9CqbnDv3r1hKtxu2bIFQKpUqQI8JolzDhw4UKxYMZg124sWLYJpYhnDff311wB+/vnnDRs2wCy3i44HDx7APPdY8RcQEYl7NAEWEVtg55LWrVsDOHbsGG9s0KABgMGDBwN44YUXAje6AGOtILZEWrlyZVBQUKBHJHENK+4kTZo00AOxat68eQC+/fZbAIsXL06bNm2gRyQiIt6hFGgRERERERGxBUWARcRG2Hdk1KhRAwYMAHD37l0AiRIlAsDSSnaOA7NQUI4cOQI9EJHAO3/+PIAMGTJAucoiInGLIsAiIiIiIiJiC4oAi4gdXbhwAcCIESMAHD16FMDYsWOh+KeIiIhInKYIsIiIiIiIiNiCIsAiIiIiIiJiC4oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImILmgCLiIiIiIiILWgCLCIiIiIiIragCbCIiIiIiIjYgibAIiIiIiIiYguaAIuIiIiIiIgtaAIsIiIiIiIitqAJsIiIiIiIiNiCJsAiIiIiIiJiC5oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImILmgCLiIiIiIiILWgCLCIiIiIiIragCbCIiIiIiIjYgibAIiIiIiIiYguaAIuIiIiIiIgtaAIsIiIiIiIitqAJsIiIiIiIiNiCJsAiIiIiIiJiC5oAi4iIiIiIiC1oAiwiIiIiIiK2oAmwiIiIiIiI2IImwCIiIiIiImIL/w9QEAFiG+v+rQAAAABJRU5ErkJggg==\" width = 400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmokwVTV8lUL"
      },
      "source": [
        "* $\\mathcal{F}$ is a periodic function for e.g. sin or cos\n",
        "* $w_i$, $\\varphi_{i}$   are learnable parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPonGx6A0UWm"
      },
      "source": [
        "Let's create a Time2Vec Layer. We need non-periodic feature and a periodic feature.\n",
        "* To your opinion is there a useless feature to exclude of this time embedding ?\n",
        "* Why ?\n",
        "\n",
        "\n",
        "\n",
        "**Important Disclaimer : Usually we add our positional encoding to our input tensor. However, in our case we will concatenate it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "iW3jL4sE9bDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fbb6c5-42e3-4407-e61f-e76fd9925a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class Time2Vector(nn.Module):\n",
        "\n",
        "  def __init__(self,in_features):\n",
        "      super().__init__()\n",
        "      self.in_features = in_features\n",
        "      self.w0 = nn.parameter.Parameter(torch.randn(1,1), requires_grad =True)\n",
        "      self.b0 = nn.parameter.Parameter(torch.randn(in_features, 1), requires_grad =True)\n",
        "      self.b0 = nn.parameter.Parameter(torch.randn(1, 1), requires_grad =True)\n",
        "\n",
        "      self.w = nn.parameter.Parameter(torch.randn(1,1), requires_grad =True)\n",
        "      self.b = nn.parameter.Parameter(torch.randn(in_features, 1), requires_grad =True)\n",
        "      self.f = torch.sin\n",
        "\n",
        "  # def forward(self,x):\n",
        "  #   # k-1 periodic features\n",
        "  #   v1 = self.f(torch.matmul(x, self.w) + self.b)\n",
        "  #   # One Non-periodic feature\n",
        "  #   v2 = torch.matmul(x, self.w0) + self.b0\n",
        "  #   return torch.cat([v1, v2], 2)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      bs,seq_len,n_feat = x.shape\n",
        "      #print(\"x SHAPE Is\", x.shape)\n",
        "\n",
        "      #print(\"BS SHAPE IS :\", bs)\n",
        "      #print(\"seq_len IS :\", seq_len)\n",
        "      #print(\"n_feat IS :\", n_feat)\n",
        "\n",
        "      # TODO : Exclude the unwanted feature and compute the mean along the last axis\n",
        "\n",
        "      # removing the last and first features\n",
        "      # first feature is the timestamp\n",
        "      # last feature is the name\n",
        "\n",
        "      x = x[:, :, 1:-1].mean(dim=-1, keepdim=True)\n",
        "\n",
        "      #print(\"x SHAPE after removing feature  Is\", x.shape)\n",
        "\n",
        "      #x = self.f * ( self.w * self.in_features  + self.b )\n",
        "\n",
        "      linear = x.unsqueeze(-1)\n",
        "      #print(\"LINEAR IS  : \", linear.shape)\n",
        "\n",
        "\n",
        "      periodic = x.unsqueeze(-1)\n",
        "\n",
        "\n",
        "      #print(\"W0 SHAPE IS : \", self.w0.shape)\n",
        "      #print(\"PERIODIC IS : \", periodic.shape)\n",
        "      #print(\"b0 SHAPE IS : \", self.b0.shape)\n",
        "      linear = torch.matmul(linear,self.w0) + self.b0\n",
        "\n",
        "      W = self.w.repeat(bs,1,1)\n",
        "      #print(\"W SHAPE IS : \", self.w.shape)\n",
        "\n",
        "\n",
        "      b = self.b.repeat(bs,1,1)\n",
        "      #print(\"b SHAPE IS : \", self.b.shape)\n",
        "\n",
        "      periodic = periodic.squeeze(-1)\n",
        "      periodic = self.f(torch.bmm(periodic,W) + b)\n",
        "\n",
        "      #print(\"periodic final SHAPE IS : \", periodic.shape)\n",
        "\n",
        "      #return torch.cat([linear , periodic], 1)\n",
        "      linear = linear.squeeze(-1)\n",
        "      return torch.cat([linear, periodic], -1).permute(0,2,1)\n",
        "\n",
        "\n",
        "      #return torch.cat([linear, periodic], -1).permute(0,2,1)\n",
        "# TODO : Verify the output of Time2Vector shape. It should be of shape (Batch Size, Sequence Length, 2)\n",
        "model_test = Time2Vector(in_features=3)\n",
        "input_tensor = torch.randn(32, 3, 5)\n",
        "output = model_test(input_tensor)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0DesvfbB6wD"
      },
      "source": [
        "### III - Transformer : A Big Model around Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZIMEQz6LZ4u"
      },
      "source": [
        "We are going to build each modules of our Transformer model. The heart of the model resides in the Attention Mecanism. The goal of the Attention mecanism is to force the model to look at specific part of the input. We will build each component of the transformer part by part.\n",
        "\n",
        "\n",
        "Create the different components of the Transformer Encoder :\n",
        "* Attention Module\n",
        "* Multi-Head Attention Module\n",
        "* Transformer Encoder Layer\n",
        "* Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1bU8e0LCUiu"
      },
      "source": [
        "#### a - Attention layer\n",
        "\n",
        "Let's compute the attention layer. We will create a layer that computes Bandhanau's attention also called Dot Scale Product attention. The attention mecanism takes an input $X$ and project it using a set of queries, keys and values. Think of it as a Database which you query (with the queries) using a set of keys, which returns a set values.\n",
        "\n",
        "Mathematicaly speaking, we are computing the scaled dot product between $Q$, $K$, $V$\n",
        "\n",
        "\n",
        "The  attention is :\n",
        "$Attention(Q,K,V)$ =  $Softmax(\\frac{Q*K}{\\sqrt{dim}})$$*V$\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" height = 400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "yoB68E2LDPBP"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, dim_query):\n",
        "    super().__init__()\n",
        "    self.dim_query = dim_query\n",
        "\n",
        "\n",
        "  def forward(self,q,k,v):\n",
        "    # TODO : Compute the attention mecanism between q,k,v\n",
        "    attn = torch.bmm(q,k)/math.sqrt(self.dim_query)\n",
        "    attn = F.softmax(attn,-1)\n",
        "    context = torch.bmm(attn,v)\n",
        "    return context, attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q24Z5fwECRji"
      },
      "source": [
        "#### b - Multi head Attention\n",
        "\n",
        "Usually, we like creating a Multi-Head Attention layer. Multi-Head only means that we are computing the attention over multiple heads. In fact, instead of having only one function computed by the attention mecanism, we leave each head free to learn a different function. Hence, we will have different outputs each computing a different value.\n",
        "\n",
        "Mathematically speaking :\n",
        "\n",
        "$MultiHead(Q,K,V)=Concat(head_1,…,head_h)W^O$\n",
        "\n",
        "with $head_i=Attention(QW_i^Q,KW_i^K,VW_i^V).$\n",
        "\n",
        "\n",
        "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTv6Bgq7bdnXdT-JDWEnnzK2EM1xY0NUEOyBg&usqp=CAU'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asj6n9anTZ0j"
      },
      "source": [
        " Question :    \n",
        " * What is $W^O$ ? Is it a learned parameter ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "cNzQxfhnFK4F"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, dim_query, dim_value, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embed dim is the shape of the feature space\n",
        "        self.embed_dim = embed_dim\n",
        "        # Num Heads is the number of heads\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.w_query = nn.Linear(embed_dim,   num_heads *  dim_query)\n",
        "        self.w_key =   nn.Linear(embed_dim,   num_heads *  dim_query)\n",
        "        self.w_value = nn.Linear(embed_dim,   num_heads *  dim_value)\n",
        "        self.linear =  nn.Linear(num_heads  * dim_value,   embed_dim)\n",
        "        # Dim Query, Dim Value are the projected dimensions of each tensors\n",
        "        self.attention = Attention(dim_query)\n",
        "\n",
        "\n",
        "  def forward(self, query, key, value):\n",
        "\n",
        "      # TODO : Project your query, key, value into their respective heads\n",
        "      q = self.w_query(query)\n",
        "      k = self.w_key(key)\n",
        "      v = self.w_value(value)\n",
        "      # TODO : Compute the attention\n",
        "      attn, context  = self.attention(  q*query   ,   k*key ,  v*value  )\n",
        "      attn = self.linear(attn)\n",
        "      return attn, context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9L27Af1C98Y"
      },
      "source": [
        "#### c - Transforming the Transformer\n",
        "\n",
        "So let's create our Transformer model. We will just create the Encoder, as we don't need the Decoder in our case. We are just trying to Encode the input and find interesting patterns.\n",
        "Usually we code the Transformer Model into a specific format :\n",
        "* Layer Class\n",
        "* Model Class\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" height=400>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "zSo7KRdQPkhS"
      },
      "outputs": [],
      "source": [
        "# TODO : Create one Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,embed_dim, dim_query, dim_value, num_heads, dim_feedforward= 256,dropout = 0.1):\n",
        "      super().__init__()\n",
        "      self.attention = MultiHeadAttention(embed_dim,\n",
        "                                          dim_query,\n",
        "                                          dim_value,\n",
        "                                          num_heads)\n",
        "\n",
        "      self.linear= nn.Sequential(nn.Linear(embed_dim, dim_feedforward),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Dropout(dropout),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Linear(dim_feedforward, embed_dim))\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(embed_dim)\n",
        "      self.norm2 = nn.LayerNorm(embed_dim)\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, sequence):\n",
        "      q=k=v= sequence.double()\n",
        "      # TODO : Compute the attention\n",
        "\n",
        "      attn, context = self.attention(q, k, v)\n",
        "\n",
        "      sequence = sequence + self.dropout1(attn)\n",
        "      sequence = self.norm1(sequence)\n",
        "      sequence = self.linear(sequence)\n",
        "      sequence = sequence +  self.dropout2(sequence)\n",
        "      sequence = self.norm2(sequence)\n",
        "      return sequence\n",
        "\n",
        "# TODO : Create a Transformer Encoder.\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, encoder_layer, num_layers):\n",
        "          super().__init__()\n",
        "          self.layers = _get_clones(encoder_layer, num_layers)\n",
        "          self.num_layers = num_layers\n",
        "\n",
        "  def forward(self, sequence):\n",
        "        output = sequence.permute(2,0,1)\n",
        "        for layer in self.layers:\n",
        "            # TODO : Send your Input to your transformer\n",
        "            output = layer(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsWa6gmqVPoM"
      },
      "source": [
        "#### Building the entire model\n",
        "\n",
        "Finally let's build the entire model. Let's use Pytorch-Lightning to encompass everything.\n",
        "Normally your model must be composed of three components :\n",
        "* The Transformer\n",
        "* The Time2Vector\n",
        "* A Regression Head\n",
        "\n",
        "As usual ask yourself what task you are performing, how your data should travle trhough the model, what the data is, blablablablalablba\n",
        "\n",
        "Don't forget to use your favorite logger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "1M5xOk8FVXtN"
      },
      "outputs": [],
      "source": [
        "class StockModel(pl.LightningModule):\n",
        "    def __init__(self,embed_dim,dim_query,dim_value, num_layers, num_heads):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        # TODO : Define your model here, be careful, your model will be an instance of the class. Watch  out for the input data.\n",
        "        # TOOD : Define your Encoder Layer\n",
        "        encoder_layer = TransformerEncoderLayer(embed_dim = embed_dim, # TODO : What should be the hidden dim ? Same as embed dim\n",
        "                                                dim_query = dim_query,\n",
        "                                                dim_value = dim_value,\n",
        "                                                num_heads = num_heads,\n",
        "                                                dim_feedforward= 256,\n",
        "                                                dropout = 0.1)\n",
        "\n",
        "        # TODO : Initialize your Transformer\n",
        "        self.transformer = TransformerEncoder(encoder_layer = encoder_layer,\n",
        "                                              num_layers = num_layers)\n",
        "\n",
        "        # TODO : Initialize your regression head\n",
        "        self.head = nn.Linear(embed_dim, 1) # What is your input hidden dim, output hidden dim ? Don't forget your time embedding\n",
        "        # TODO : Initialize your Time2Vector Embeddings\n",
        "        self.timeencoder = Time2Vector(in_features= 3) # What is the in_features dimension ? Is it the same as the input ?\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        assert isinstance(x, torch.Tensor), \"Input must be a PyTorch tensor\"\n",
        "\n",
        "        bs, h, len = x.shape\n",
        "\n",
        "        pos = x.permute(0,2,1)\n",
        "        #print(\"x shape is: \", x.size())\n",
        "\n",
        "        time_vec = self.timeencoder(pos)\n",
        "\n",
        "        # TODO : Concatenate your time embedding to the input sequence\n",
        "        x = torch.cat((x, time_vec), dim=1)\n",
        "\n",
        "\n",
        "        #x = torch.cat([x, time_vec.unsqueeze(-1)], dim=1) # TODO : Verify that the tensor was correctly concatenated within the feature dim\n",
        "\n",
        "        # TODO : Send your input through your transformer\n",
        "        x = self.transformer(x)\n",
        "        x = x.view(bs*len, -1)\n",
        "        # TODO : Send your input through the regression head\n",
        "        x = self.head(x)\n",
        "        x = x.view(bs,-1,len)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Training Step\n",
        "        X = batch[\"input\"]\n",
        "        y = batch[\"label\"]\n",
        "        out = self(X)\n",
        "        out = out.mean()\n",
        "        y = y.mean()\n",
        "        #loss = nn.MSELoss(out, y)\n",
        "        # Don't remove the next line, you will understand why later\n",
        "\n",
        "        loss = F.mse_loss(out, y)\n",
        "\n",
        "        self.log('train_loss', loss)\n",
        "        #return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Validation Step\n",
        "        X_val = batch[\"input\"]\n",
        "        y_val = batch[\"label\"]\n",
        "\n",
        "        out_val = self(X_val)\n",
        "        out_val = out_val.mean()\n",
        "        y_val = y_val.mean()\n",
        "\n",
        "        loss = F.mse_loss(out_val, y_val)\n",
        "\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Test Step\n",
        "        X = batch[\"input\"]\n",
        "        y = batch[\"label\"]\n",
        "        out = self(X)\n",
        "        loss = F.mse_loss(out, y)\n",
        "        self.log('test_loss', loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNs5inW6Xt_a"
      },
      "source": [
        "## IV - Training the Model\n",
        "\n",
        "* Initialize a model with 3 stacks of Encoder with 8 heads.\n",
        "* What is the Embed Dimension ?\n",
        "* What is the Dimension of a Query, Key and Value ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "8HXr5PLUXupy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9da93d42-220f-46d7-f789-6307c6c01067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "# TODO : Initalize Model, Datamodule and Trainer\n",
        "embed_dim = 7\n",
        "num_heads = 1\n",
        "dim_query = 7\n",
        "dim_value = 7\n",
        "num_layers = 1\n",
        "\n",
        "model = StockModel(embed_dim=embed_dim,\n",
        "                      dim_query=dim_query,\n",
        "                      dim_value=dim_value,\n",
        "                      num_layers=num_layers,\n",
        "                      num_heads= num_heads).double()\n",
        "\n",
        "dm = StockDataModule(df = df, N_window =3, normalized=True, batch_size=7)\n",
        "\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator='cuda',\n",
        "    max_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "DPXG6HUEe2Qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633,
          "referenced_widgets": [
            "b1907c710e594a26a079b017b210c7e1",
            "30e85978491d435b9e44c0fa0713ae42",
            "e7a2fc7c6acb46968b0f1c2547d1700d",
            "dc6d022eacc04cf7ae095f7072025790",
            "5433df94935e44fa9913c00defa8f3a4",
            "7031737635574c149ec14e01934edd3a",
            "80059eb485434180a6b57ad921cd967b",
            "d76201eec4d04b858241ee39ea51e961",
            "5f469cfeeaea4834bba8302236e427b6",
            "084ffa683f514359befd1993ff6e5955",
            "402fc4e934214ababec2f74d10bec423",
            "4c0f5c67e57248dab04eb51b91e66c07",
            "271a33825a9f4ab2984d3871a69ada81",
            "78074c22685e4c56ad4cb6d46f8ae83b",
            "bf3c34399f824c85abac8890081a8fd3",
            "77f69e2024694e59a611ae5d97739f74",
            "f00c716c0a8a4325bb33ac985c4ad3a9",
            "08bb170811054ac29bc2c87780841aca",
            "43ccf1b5c0f640bb8920abab4ccb6bc5",
            "75f1bf768cb24e51961316a562fe0446",
            "7601064492bb42749159fec540c185e2",
            "96217591839f42f8862a01ade367cbae"
          ]
        },
        "outputId": "3e4094e8-7ff9-4e3d-cef2-512919a89f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name        | Type               | Params\n",
            "---------------------------------------------------\n",
            "0 | transformer | TransformerEncoder | 4.1 K \n",
            "1 | head        | Linear             | 8     \n",
            "2 | timeencoder | Time2Vector        | 6     \n",
            "---------------------------------------------------\n",
            "4.1 K     Trainable params\n",
            "0         Non-trainable params\n",
            "4.1 K     Total params\n",
            "0.016     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1907c710e594a26a079b017b210c7e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c0f5c67e57248dab04eb51b91e66c07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-234-5d71765bec7e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO : Fit the Data to the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         )\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;34m\"optimizer_step\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \"\"\"\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_model_and_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     def _clip_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclosure_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mClosureResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-332923365439>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-332923365439>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# TODO : Send your input through your transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# TODO : Send your input through the regression head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-193-bef458235507>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# TODO : Send your Input to your transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-193-bef458235507>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;31m# TODO : Compute the attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-192-4da1b2072757>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;31m# TODO : Compute the attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0mq\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mquery\u001b[0m   \u001b[0;34m,\u001b[0m   \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0mv\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalue\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-191-36b62cb71288>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# TODO : Compute the attention mecanism between q,k,v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [3, 7] but got: [3, 4]."
          ]
        }
      ],
      "source": [
        "# TODO : Fit the Data to the Model\n",
        "trainer.fit(model, dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_21I4eFCTqtH"
      },
      "source": [
        "## V - Testing the model : Inference\n",
        "\n",
        "Now that the model is trained, testing it is a key to become rich."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h70XsCUhZWdr"
      },
      "source": [
        "#### a - Testings\n",
        "\n",
        "Test the model on the test dataset.\n",
        "\n",
        "* What happens ?\n",
        "* What can we do to enhance the results ?\n",
        "* Will you deploy the model ?\n",
        "* What are your predictions for next week ? Can we invest or not ?\n",
        "\n",
        "**Illustrate your arguments using charts or any kind of visual materials supporting your analysis. Any non illustrated analysis won't be taken into account**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "2LemCOfzERLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "78c12eb5059249739ca1d686f36618c1",
            "9fcbb11f27bd4e2bb70e8660bf18ad02",
            "09f2c17b6f9d41f5aee2316241a0b1fb",
            "ebf05855220341a2895bad90e47c522e",
            "436154df4b6647cf9ae5d0e7553560ae",
            "45c261976b7741919917d0e6253ac6e6",
            "c5f0e8270b964cd8a10f6276fb1a2e64",
            "e34c26f677c541ae933d880609537cb5",
            "490a792f7a0b44ee8104324ecaa81b10",
            "5eb577c79f2d490c9c3d8c681062d846",
            "b91ec519fd9f4deba7df30420caf50b3"
          ]
        },
        "outputId": "a9d7ec5d-2a30-4971-ae4a-dd56518b15ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=1` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78c12eb5059249739ca1d686f36618c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-194-332923365439>:92: UserWarning: Using a target size (torch.Size([7, 5, 7])) that is different to the input size (torch.Size([7, 1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.mse_loss(out, y)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-233-70c6a801fa1f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;31m# remove the tensors from the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tensors_to_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         )\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstep_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-332923365439>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3326\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3328\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (7) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "dm.setup(stage='test')\n",
        "test = dm.test_dataloader()\n",
        "\n",
        "trainer.test(model, dataloaders =test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtli_XCEZJL_"
      },
      "source": [
        "#### b - Further Testings\n",
        "\n",
        "Test the model on a different dataset.\n",
        "* What can you say ?\n",
        "* What are your predictions for next week ? Can we invest or not ?\n",
        "\n",
        "**Illustrate your arguments using charts or any kind of visual materials supporting your analysis. Any non illustrated analysis won't be taken into account**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZtp7Aq7ZS2m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1907c710e594a26a079b017b210c7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30e85978491d435b9e44c0fa0713ae42",
              "IPY_MODEL_e7a2fc7c6acb46968b0f1c2547d1700d",
              "IPY_MODEL_dc6d022eacc04cf7ae095f7072025790"
            ],
            "layout": "IPY_MODEL_5433df94935e44fa9913c00defa8f3a4"
          }
        },
        "30e85978491d435b9e44c0fa0713ae42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7031737635574c149ec14e01934edd3a",
            "placeholder": "​",
            "style": "IPY_MODEL_80059eb485434180a6b57ad921cd967b",
            "value": "Sanity Checking: "
          }
        },
        "e7a2fc7c6acb46968b0f1c2547d1700d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76201eec4d04b858241ee39ea51e961",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f469cfeeaea4834bba8302236e427b6",
            "value": 0
          }
        },
        "dc6d022eacc04cf7ae095f7072025790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084ffa683f514359befd1993ff6e5955",
            "placeholder": "​",
            "style": "IPY_MODEL_402fc4e934214ababec2f74d10bec423",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "5433df94935e44fa9913c00defa8f3a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "7031737635574c149ec14e01934edd3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80059eb485434180a6b57ad921cd967b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d76201eec4d04b858241ee39ea51e961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f469cfeeaea4834bba8302236e427b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "084ffa683f514359befd1993ff6e5955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "402fc4e934214ababec2f74d10bec423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c0f5c67e57248dab04eb51b91e66c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_271a33825a9f4ab2984d3871a69ada81",
              "IPY_MODEL_78074c22685e4c56ad4cb6d46f8ae83b",
              "IPY_MODEL_bf3c34399f824c85abac8890081a8fd3"
            ],
            "layout": "IPY_MODEL_77f69e2024694e59a611ae5d97739f74"
          }
        },
        "271a33825a9f4ab2984d3871a69ada81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f00c716c0a8a4325bb33ac985c4ad3a9",
            "placeholder": "​",
            "style": "IPY_MODEL_08bb170811054ac29bc2c87780841aca",
            "value": "Epoch 0: 100%"
          }
        },
        "78074c22685e4c56ad4cb6d46f8ae83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43ccf1b5c0f640bb8920abab4ccb6bc5",
            "max": 28182,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75f1bf768cb24e51961316a562fe0446",
            "value": 28180
          }
        },
        "bf3c34399f824c85abac8890081a8fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7601064492bb42749159fec540c185e2",
            "placeholder": "​",
            "style": "IPY_MODEL_96217591839f42f8862a01ade367cbae",
            "value": " 28180/28182 [02:32&lt;00:00, 184.80it/s, v_num=59]"
          }
        },
        "77f69e2024694e59a611ae5d97739f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "f00c716c0a8a4325bb33ac985c4ad3a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08bb170811054ac29bc2c87780841aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43ccf1b5c0f640bb8920abab4ccb6bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f1bf768cb24e51961316a562fe0446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7601064492bb42749159fec540c185e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96217591839f42f8862a01ade367cbae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78c12eb5059249739ca1d686f36618c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fcbb11f27bd4e2bb70e8660bf18ad02",
              "IPY_MODEL_09f2c17b6f9d41f5aee2316241a0b1fb",
              "IPY_MODEL_ebf05855220341a2895bad90e47c522e"
            ],
            "layout": "IPY_MODEL_436154df4b6647cf9ae5d0e7553560ae"
          }
        },
        "9fcbb11f27bd4e2bb70e8660bf18ad02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c261976b7741919917d0e6253ac6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f0e8270b964cd8a10f6276fb1a2e64",
            "value": "Testing DataLoader 0:   0%"
          }
        },
        "09f2c17b6f9d41f5aee2316241a0b1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e34c26f677c541ae933d880609537cb5",
            "max": 12525,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_490a792f7a0b44ee8104324ecaa81b10",
            "value": 0
          }
        },
        "ebf05855220341a2895bad90e47c522e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eb577c79f2d490c9c3d8c681062d846",
            "placeholder": "​",
            "style": "IPY_MODEL_b91ec519fd9f4deba7df30420caf50b3",
            "value": " 0/12525 [00:00&lt;?, ?it/s]"
          }
        },
        "436154df4b6647cf9ae5d0e7553560ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "45c261976b7741919917d0e6253ac6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f0e8270b964cd8a10f6276fb1a2e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e34c26f677c541ae933d880609537cb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490a792f7a0b44ee8104324ecaa81b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5eb577c79f2d490c9c3d8c681062d846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91ec519fd9f4deba7df30420caf50b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}